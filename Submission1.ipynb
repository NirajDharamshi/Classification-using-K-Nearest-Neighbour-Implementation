{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ou4Y2F6RlN3Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import contractions\n",
    "#import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import f1_score\n",
    "import math\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "foE5g_C3lN3X"
   },
   "outputs": [],
   "source": [
    "# read in the dataset\n",
    "df1 = pd.read_csv('train.dat',header=None,sep='\\t')\n",
    "df2 = pd.read_csv('test.dat',header=None,sep='\\t')\n",
    "df2.insert(0,'cls',0)\n",
    "df2.columns = [0, 1]\n",
    "df=df1.append(df2,ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLuaQfUKlN3b"
   },
   "outputs": [],
   "source": [
    "# separate names from classes\n",
    "vals = df.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufsRzU3GlN3e"
   },
   "outputs": [],
   "source": [
    "#dividing into classes and abstract called as 'names'\n",
    "cls=[]\n",
    "names=[]\n",
    "for i in range(len(vals)):\n",
    "    names.append(vals[i][1])\n",
    "    if (vals[i][0])==0:\n",
    "        continue\n",
    "    cls.append(vals[i][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnxfcp81lN3i"
   },
   "outputs": [],
   "source": [
    "border=len(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Abrl8F12lN3m"
   },
   "outputs": [],
   "source": [
    "#defining normalising functions\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSh5KIo7lN3o"
   },
   "outputs": [],
   "source": [
    "#breaking sentences into words 'tokenizing'\n",
    "for i in range(len(vals)):\n",
    "    names[i]=nltk.word_tokenize(names[i])\n",
    "#print(names[774])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMcqcn6_lN3s"
   },
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "for i in range(len(vals)):\n",
    "    names[i]=remove_punctuation(names[i])\n",
    "#print(names[963])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luOQEE8qlN3w"
   },
   "outputs": [],
   "source": [
    "#removing stop words like 'the','a' etc\n",
    "for i in range(len(vals)):\n",
    "    names[i]=remove_stopwords(names[i])\n",
    "#print(names[963])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHMG71oWlN3z"
   },
   "outputs": [],
   "source": [
    "#lemmatizing words\n",
    "for i in range(len(vals)):\n",
    "    names[i]=lemmatize_verbs(names[i])\n",
    "#print(names[963])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2F4GWQrlN34"
   },
   "outputs": [],
   "source": [
    "#stemming \n",
    "for i in range(len(vals)):\n",
    "    names[i]=stem_words(names[i])\n",
    "#print(names[963])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2SMV4E1lN38"
   },
   "outputs": [],
   "source": [
    "#converting into lower case\n",
    "for i in range(len(vals)):\n",
    "    names[i]=to_lowercase(names[i])\n",
    "#print(names[774])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HwlMnaelN4B"
   },
   "outputs": [],
   "source": [
    "#csr matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "   \n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=np.int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=np.int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d )\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPPNqGzLlN4G"
   },
   "outputs": [],
   "source": [
    "#csr information\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZnzmO7elN4K"
   },
   "outputs": [],
   "source": [
    "#normalise csr\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKHqxGtrlN4O"
   },
   "outputs": [],
   "source": [
    "def classifyNames(names, cls,k=10):\n",
    "   \n",
    "    docs = names\n",
    "    mat = build_matrix(docs)\n",
    "    csr_l2normalize(mat)\n",
    "    train=mat[:border-1,:]\n",
    "    test=mat[border:,:]\n",
    "    \n",
    "    \n",
    "    def classify(x, train, clstr):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "        dots = x.dot(train.T)\n",
    "        #dots[0] = -1 # invalidate self-similarity\n",
    "        #denom1=denom(x)\n",
    "        #for i in range(train.shape[0]):\n",
    "         #   dots[0,i]=dots[0,i]/denom1\n",
    "          #  y=train[i,:].toarray()[0]\n",
    "           # denom2=denom(y)\n",
    "            #dots[0,i]=dots[0,i]/denom2\n",
    "        \n",
    "        \n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        #n = []\n",
    "        #x1=x.toarray()[0]\n",
    "        #for i in range(train.shape[0]):\n",
    "         #   y=train[i,:].toarray()[0]\n",
    "          #  n.append(cosine_similarity(x1,y))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return '1' if np.random.rand() > 0.5 else '5'\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        #a=sorted(range(len(n)), key=lambda i: n[i], reverse=True)[:k]\n",
    "       # wsum=0.0\n",
    "       # wdenom=0.0\n",
    "       # for i in sims[:k]:\n",
    "        #    wsum+=clstr[s[0]]*dots[s[0]]\n",
    "         #   wdenom+=dots[s[0]]\n",
    "        #clpred=int(wsum/wdenom)\n",
    "        #-----------------------------------------------------\n",
    "        #wsum=0.0\n",
    "        #wdenom=0.0\n",
    "        #clpred=0.0\n",
    "        #p=[clstr[s[0]] for s in sims[:k] if s[1] > 0 ]\n",
    "        #q=[dots[0,s[0]] for s in sims[:k] if s[1] > 0 ]\n",
    "        #for i in range(k):\n",
    "          #  wsum+=p[i]*q[i]\n",
    "         #   wdenom+=q[i]\n",
    "        #clpred=wsum/wdenom\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(5)\n",
    "        if len(tc) < 6 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        #if len(tc) < 5 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "         #   return tc[0][0]\n",
    "        # tie break\n",
    "        #tc = defaultdict(float)\n",
    "        #for s in a[:k]:\n",
    "         #   tc[clstr[s[0]]] += s[1]\n",
    "        #return clpred\n",
    "        \n",
    "    macc = 0.0\n",
    "    \n",
    "    #for f in range(d):\n",
    "        # split data into training and testing\n",
    "     #   train, clstr, test, clste = splitData(mat, cls, f+1, d)\n",
    "        # predict the class of each test sample\n",
    "    clspr = [ classify(test[i,:], train, cls) for i in range(test.shape[0]) ]\n",
    "        # compute the accuracy of the prediction\n",
    "       # acc = 0.0\n",
    "        #for i in range(len(clste)):\n",
    "         #   if clste[i] == clspr[i]:\n",
    "          #      acc += 1\n",
    "        #acc /= len(clste)\n",
    "        #macc += acc\n",
    "        \n",
    "    \n",
    "    #score=f1_score(clste, clspr, average='micro')\n",
    "        \n",
    "    return clspr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s20K3SHxlN4R"
   },
   "outputs": [],
   "source": [
    "#classification file\n",
    "x=classifyNames(names,cls,30)\n",
    "with open('format1.dat','w') as filehandle:\n",
    "    for listitem in x:\n",
    "        filehandle.write('%d\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQ_yfhQD74Rf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4AvEFSslN4T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Submission1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
